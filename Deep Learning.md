[TOC]

## 神经网络和深度学习

### 什么是神经网络？

常用深度学习这个术语来指训练神经网络的过程。

从趋近于零开始，然后变成一条直线，这个函数被称为 ReLU 激活函数，全称是 Rectified Linear Unit（修正线性单元）。Rectify 可以理解成 max(0,x)。

神经网络的一部分神奇之处在于，当你实现它之后，要做的只是输入 x，就能得到输出 y。因为它可以自己计算训练集中样本的数目以及所有的中间过程。输入层和中间层被紧密地连接起来。



### 神经网络的监督学习

神经网络有很多的种类，但事实表明，到目前几乎所有由神经网络创造的经济价值，本质上都离不开一种叫做监督学习的机器学习类别。

对于图像应用，经常在神经网络上使用卷积（Convolutional Neural Network），缩写为 CNN。

递归神经网络（RNN）非常适合一维序列，数据可能是一个时间组成部分。

结构化数据意味着数据的基本数据库，即每个特征都有一个很好的定义。而非结构化数据，比如音频、图像、文本等。这里的特征可能是图像中的像素值或文本中的单词。



### 为什么深度学习会兴起？

我们收集到了大量的数据，远超过机器学习算法能够高效发挥它们优势的规模。

如果想要获得较高的性能体现，有两个条件要完成。一是需要训练一个足够大的神经网络，以发挥数据规模巨大的优点；二是需要很多的数据。



## 神经网络的编程基础

### 二分类

逻辑回归（logistic regression）是一个用于二分类（binary classification）的算法。

为了保存一张图片，需要保存三个矩阵，它们分别对应图片中的红、绿、蓝三种颜色通道。把这些像素值提取出来，然后放入一个特征向量 x。常用 n 来表示输入特征向量 x 的维度。所以，在二分类问题中，目标就是习得一个分类器，以图片的特征向量作为输入，然后预测输出结果 y 为 1 还是 0。

将样本数据堆叠在矩阵的列中，会更加容易地实现一个神经网络。

X 是一个规模为 n 乘以 m 的矩阵。当使用 Python 实现时，X.shape 用于显示矩阵的规模，等于(n,m)。Y 是一个规模为 1 乘以 m 的矩阵。



### 逻辑回归

用 w 来表示逻辑回归的参数，也是一个 n 维向量（因为 w 实际上是特征权重，维度与特征向量相同），参数中还有 b，表示偏差。输出 y'，是对实际值 y 的估计，或者说表示 y 等于 1 的一种可能性。

尝试 y'=w(转置)x+b，这是在做线性回归时所用到的，但这对于二元分类问题不是一个非常好的算法，因为 y' 应该在 0 到 1 之间。因此，在逻辑回归中，输出 y' 等于由上面得到的线性函数式子作为自变量的 sigmoid 函数，将线性函数转换为非线性函数。



### 逻辑回归的代价函数

通过训练代价函数来得到参数 w 和参数 b。

损失函数又叫做误差函数，用来衡量算法的运行情况，Loss function：L(y',y)。一般我们用预测值和实际值的平方差或平方差的一半来衡量，但通常在逻辑回归中不这么做，因为逻辑回归参数的优化目标不是凸优化。

在这门课中有很多的函数效果和这个类似，如果 y 等于 1，就尽可能让 y' 变大；如果 y 等于 0，就尽可能让 y' 变小。

损失函数是在单个训练样本中定义的，因而需要定义一个算法的代价函数 J(w,b)，即对 m 个样本的损失函数求和然后除以 m。



### 梯度下降法

由于逻辑回归的代价函数特性，我们必须定义其为凸函数。

~~~
初始化 w 和 b。对于逻辑回归几乎所有的初始化方法都有效，因为函数是凸函数，无论在哪里初始化，应该达到同一点或大致相同的点。
朝最陡的下坡方向走一步，不断迭代。
直到走到全局最优解或接近全局最优解的地方。
~~~



### 计算图

一个神经网络的计算，都是按照前向或反向传播过程组织的。首先，计算出一个新的网络的输出（前向过程），紧接着进行一个反向传输操作，后者用来计算对应的梯度或导数。



### 计算图的导数计算

链式法则

在程序里 dvar 表示导数，最终变量对各种中间量的导数。



### 逻辑回归中的梯度下降

全局代价函数对 w 的微分是各项损失对 w 微分的平均。

~~~python
J=0;dw1=0;dw2=0;db=0;
for i=1 to m
	z(i)=wx(i)+b;
	a(i)=sigmoid(z(i));
	J+=-[y(i)log(a(i))+(1-y(i))log(1-a(i))];
	dz(i)=a(i)-y(i);
	dw1+=x1(i)dz(i);
	dw2+=x2(i)dz(i);
	db+=dz(i);
J/=m;
dw1/=m;
dw2/=m;
db/=m;
w=w-alpha*dw;
b=b-alpha*db
~~~

这种计算中有两个缺点，也就是需要编写两个 for 循环。第一个 for 循环是一个小循环遍历 m 个训练样本，第二个 for 循环是遍历所有特征。

应用深度学习算法，会发现在代码中显示地使用 for 循环导致算法很低效。所以向量化技术，可以允许摆脱这些显示的 for 循环。



### 向量化



