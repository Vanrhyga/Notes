[TOC]

## 神经网络和深度学习

### 什么是神经网络？

常用深度学习这个术语来指训练神经网络的过程。

从趋近于零开始，然后变成一条直线，这个函数被称为 ReLU 激活函数，全称是 Rectified Linear Unit（修正线性单元）。修正，指的是取不小于 0 的值，Rectify 可以理解成 max(0,x)。

每个第一层神经元的输入都来自所有特征。

神经网络的一部分神奇之处在于，当你实现它之后，要做的只是输入 x，就能得到输出 y。因为它可以自己计算训练集中样本的数目以及所有的中间过程。输入层和中间层被紧密地连接起来。



### 神经网络的监督学习

神经网络有很多的种类，但事实表明，到目前几乎所有由神经网络创造的经济价值，本质上都离不开一种叫做监督学习的机器学习类别。

对于图像应用，经常在神经网络上使用卷积（Convolutional Neural Network），缩写为 CNN。

递归神经网络（RNN）非常适合一维序列，数据可能是一个时间组成部分。

结构化数据意味着数据的基本数据库，即每个特征都有一个很好的定义。而非结构化数据，比如音频、图像、文本等。这里的特征可能是图像中的像素值或文本中的单词。



### 为什么深度学习会兴起？

我们收集到了大量的数据，远超过机器学习算法能够高效发挥它们优势的规模。

如果想要获得较高的性能体现，有两个条件要完成。一是需要训练一个足够大的神经网络，以发挥数据规模巨大的优点；二是需要很多的数据。



## 神经网络的编程基础

### 二分类

逻辑回归（logistic regression）是一个用于二分类（binary classification）的算法。

为了保存一张图片，需要保存三个矩阵，它们分别对应图片中的红、绿、蓝三种颜色通道。把这些亮度像素值提取出来，然后放入一个特征向量 x。常用 n 来表示输入特征向量 x 的维度。所以，在二分类问题中，目标就是习得一个分类器，以图片的特征向量作为输入，然后预测输出结果 y 为 1 还是 0。

将样本数据堆叠在矩阵的列中，会更加容易地实现一个神经网络。

X 是一个规模为 n 乘以 m 的矩阵。当使用 Python 实现时，X.shape 用于显示矩阵的规模，等于(n,m)。Y 是一个规模为 1 乘以 m 的矩阵。



### 逻辑回归

用 w 来表示逻辑回归的参数，也是一个 n 维向量（因为 w 实际上是特征权重，维度与特征向量相同），参数中还有 b，表示偏差。输出 y'，是对实际值 y 的估计，或者说表示 y 等于 1 的一种可能性。

尝试 y'=w(转置)x+b，这是在做线性回归时所用到的，但这对于二元分类问题不是一个非常好的算法，因为 y' 应该在 0 到 1 之间。因此，在逻辑回归中，输出 y' 等于由上面得到的线性函数式子作为自变量的 sigmoid 函数，将线性函数转换为非线性函数。



### 逻辑回归的代价函数

通过训练代价函数来得到参数 w 和参数 b。

损失函数又叫做误差函数，用来衡量算法的运行情况，Loss function：L(y',y)。一般我们用预测值和实际值的平方差或平方差的一半来衡量，但通常在逻辑回归中不这么做，因为在后期的优化中，它会变成非凸的，从而得到很多个局部最优解或者说梯度下降法，可能找不到全局最优解。

在这门课中有很多的函数效果和这个类似，如果 y 等于 1，就尽可能让 y' 变大；如果 y 等于 0，就尽可能让 y' 变小。

损失函数是在单个训练样本中定义的，因而需要定义一个算法的代价函数 J(w,b)，即对 m 个样本的损失函数求和然后除以 m。



### 梯度下降法

由于逻辑回归的代价函数特性，我们必须定义其为凸函数。

~~~
初始化 w 和 b。对于逻辑回归几乎所有的初始化方法都有效，因为函数是凸函数，无论在哪里初始化，应该达到同一点或大致相同的点。
朝最陡的下坡方向走一步，不断迭代。
直到走到全局最优解或接近全局最优解的地方。
~~~



### 计算图

一个神经网络的计算，都是按照前向或反向传播过程组织的。首先，计算出一个新的网络的输出（前向过程），紧接着进行一个反向传输操作，后者用来计算对应的梯度或导数。



### 计算图的导数计算

链式法则

在程序里 dvar 表示导数，最终变量对各种中间量的导数。



### 逻辑回归中的梯度下降

全局代价函数对 w 的微分是各项损失对 w 微分的平均。

~~~python
J=0;dw1=0;dw2=0;db=0;
for i=1 to m
	z(i)=wx(i)+b;
	a(i)=sigmoid(z(i));
	J+=-[y(i)log(a(i))+(1-y(i))log(1-a(i))];
	dz(i)=a(i)-y(i);
	dw1+=x1(i)dz(i);
	dw2+=x2(i)dz(i);
	db+=dz(i);
J/=m;
dw1/=m;
dw2/=m;
db/=m;
w=w-alpha*dw;
b=b-alpha*db
~~~

这种计算中有两个缺点，也就是需要编写两个 for 循环。第一个 for 循环是一个小循环遍历 m 个训练样本，第二个 for 循环是遍历所有特征。

应用深度学习算法，会发现在代码中显示地使用 for 循环导致算法很低效。所以向量化技术，可以允许摆脱这些显示的 for 循环。



### 向量化

~~~python
z=np.dot(w,x)+b
~~~

~~~python
import numpy as np	#导入 numpy 库
a=np.array([1,2,3,4])	#创建一个数据 a
print(a)
#[1 2 3 4]

import time		#导入时间库
a=np.random.rand(1000000)
b=np.random.rand(1000000)	#通过 round 随机得到两个一百万维度的数组
tic=time.time()		#现在测量一下当前时间

#向量化的版本
c=np.dot(a,b)
toc=time.time()
print("Vectorized version:"+str(1000*(toc-tic))+"ms")	#打印向量化版本的时间

#继续增加非向量化的版本
c=0
tic=time.time()
for i in range(1000000):
	c+=a[i]*b[i]
toc=time.time()
print(c)
print("For loop:"+str(1000*(toc-tic))+"ms")		#打印 for 循环版本的时间
~~~

如果使用了 built-in 函数，像 np.function 或并不要求实现循环的函数，可以让 python 充分利用并行化计算。

GPU 更加擅长 SIMD 计算。

经验法则是，无论什么时候，避免使用明确的 for 循环。

如果有一个向量 v，并且想要对向量 v 的每个元素做指数操作，从而得到向量 u。首先，初始化向量 u=np.zeros(n,1)，并且通过循环依次计算每个元素。但事实可以通过 python 的 numpy 内置函数计算，u=np.exp(v)。

numpy 库有很多向量函数。如 u=np.log 是计算对数函数，np.abs() 是计算数据的绝对值，np.maximum() 计算元素中最大值，也可以用 np.maximum(v,0)，v**2 代表获得每个元素的平方。



### 向量化逻辑回归

Z=np.dot(w.T,X)+b。python 中有个巧妙的地方，这里 b 是一个实数，但是当向量加上这个实数时，会自动扩展成 1xm 的行向量，这被称为广播。



### 向量化逻辑回归的梯度输出

~~~
db=1/m*np.sum(dZ)
dw=1/m*X*dZ.T
~~~



### python 中的广播

~~~python
import numpy as np
A=np.array([[56.0,0.0,4.4,68.0],[1.2,104.0,52.0,8.0],[1.8,135.0,99.0,0.9]])
print(A)

cal=A.sum(axis=0)
print(cal)

percentage=100*A/cal.reshape(1,4)
print(percentage)
~~~

axis 用来指明将要进行的运算是沿哪个轴执行，在 numpy 中，0 轴是垂直的，而 1 轴是水平的。

技术上来讲，不需要再将矩阵 cal 重塑成 1x4。但是当我们不确定矩阵维度时，通常会对矩阵进行重塑来确保得到想要的列向量或行向量。reshape 是个常量时间的操作，调用代价极低。

广播机制与执行的运算种类无关。

如果两个数组的后缘维度的轴长度相符或其中一方的轴长度为 1，则认为它们是广播兼容的。广播会在缺失维度和轴长度为 1 的维度上进行。

后缘维度的轴长度：A.shape[-1]，即矩阵维度元组中最后一个位置的值。



### 关于numpy 向量的说明

如果将一个列向量添加到一个行向量中，你以为会报出维度不匹配或类型错误之类的错误，但实际上会得到一个行向量和列向量的求和。

~~~python
import numpy as np
a=np.random.randn(5)
print(a)	#一维数组，既不是一个行向量，也不是一个列向量

print(a.shape)

print(a.T)	#和 a 相同

print(np.dot(a,a.T))	#得到一个数
~~~

建议编写神经网络时，不要在它的结构是一维数组时使用数据结构。

~~~python
a=np.random.randn(5,1)
print(a)

print(a.T)	#行向量

print(np.dot(a,a.T))	#返回一个矩阵
~~~

每次创建一个数组，都让它成为一个列向量或行向量，其行为会更容易被理解。

使用 assert() 确保它是一个向量。





## 浅层神经网络

### 神经网络的表示

输入特征 x1、x2、x3等，被竖直地堆叠起来，叫做神经网络的输入层。最后一层只由一个结点构成，称为输出层，负责产生预测值。隐藏层的含义：在训练集中，中间结点的准确值是不知道的，也就是说看不到它们在训练集中应具有的值。

alpha表示激活，它意味着网络中不同层的值会传递到它们后面的层中，即每个神经元的输出。

计算网络的层数时，输入层不算入总层数。常将输入层称为第零层。



### 计算一个神经网络的输出

向量化的过程是将神经网络中的一层神经元参数纵向堆积起来。



### 多样本向量化

在垂直方向，索引对应于神经网络中的不同结点。水平方向上，对应于不同的训练样本。



### 激活函数

tanh 函数是 sigmoid 向下平移和伸缩后的结果，使其穿过（0,0），并且值域介于 +1 和 -1 之间。

结果表明，如果在隐藏层上使用 tanh 效果总是优于 sigmoid 函数。因为函数值域在 -1 和 +1 的激活函数，其均值是更接近 0 的，而不是 0.5。

但有一个例外：在二分类问题中，对于输出层，因为 y 的值是 0 或 1，所以需要使用 sigmoid 激活函数。

在不同的神经网络层中，激活函数可以不同。

sigmoid 函数和 tanh 函数共同的缺点是：在 z 特别大或特别小的情况下，导数的梯度会变得特别小，最后接近于 0，导致梯度下降的速度降低。

有一些选择激活函数的经验法则：如果输出是 0、1值（二分类问题），则输出层选择 sigmoid 函数，其他的所有单元都选择 ReLU 函数。

如果在隐藏层上不确定使用哪个激活函数，通常会使用 ReLU 激活函数。有时，也会使用 tanh 激活函数，但 ReLU 的一个优点是：当 z 是负值的时候，导数等于 0。

ReLU 和 Leaky ReLU函数的优点是：第一，程序实现是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLU 激活函数神经网络通常会比使用 sigmoid 或 tanh 学习得更快；第二，sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，从而造成梯度弥散。（ReLU 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性。而 Leaky ReLU 不会有这种问题）

z 在 ReLU 的梯度一半都是 0，但是，有足够的隐藏层使得 z 值大于 0，所以对大多数的训练数据来说学习过程仍然可以很快。

~~~
sigmoid 函数：除了输出层是二分类问题基本不会用它
tanh 函数：非常优秀，几乎适合所有场合
ReLU 函数：最常用的默认函数
~~~

建议：如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或发展集上进行评价。如果仅仅遵守使用默认的 ReLU 激活函数，那就很可能在近期或往后，每次解决问题都使用相同的办法。



### 为什么需要非线性激活函数？

如果使用线性激活函数或没有使用一个激活函数，那么无论神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。

只有一个地方可以使用线性激活函数，就是在做机器学习中的回归问题。

总之，不能在隐藏层用线性激活函数，唯一可以用线性激活函数的通常就是输出层；除了这种情况，在隐层用线性函数的，比如与压缩有关的，将不深入讨论。



### 随机初始化

对于一个神经网络，如果把权重或参数都初始化为 0，那么梯度下降将不会起作用。

~~~
W(1)=np.random.randn(2,2)*0.01
b(1)=np.zeros((2,1))
W(2)=np.random.randn(2,2)*0.01
~~~

通常倾向于初始化为很小的随机数，因为这种情况下很可能停在 tanh/sigmoid 函数平坦的地方。这些地方梯度很小也就意味着下降会很慢，因此学习也就很慢。





## 深层神经网络

### 为什么使用深层表示？

深度神经网络的许多隐藏层中，较早的前几层能学习一些低层次的简单特征，等到后几层，就能把简单的特征结合起来，去探测更加复杂的东西。

深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量。



###参数 vs 超参数

什么是超参数？比如算法中的 learning rate（学习率）、iterations（梯度下降法循环的数量）、L（隐藏层数目）、隐藏层单元数目、choice of activation function（激活函数的选择）都需要人为设置。这些数字实际上控制了最后的参数 W 和 b 的值，所以被称作超参数。

如何寻找超参数的最优值？走 Idea-Code-Experiment-Idea 这个循环，尝试各种不同的参数，实现模型并观察是否成功，然后再迭代。





## 改善深层神经网络：超参数调试、正则化以及优化

### 训练，验证，测试集

开始对训练集执行算法，通过验证集或简单交叉验证集选择最好的模型，然后在测试集上进行评估。

小数据量时代，常见做法是将所有数据三七分，即 70% 训练集，30% 测试集；也可以按照 60% 训练集，20% 验证集和 20% 测试集来划分。

大数据时代，假设我们有 100 万条数据，训练集占 98%，验证集和测试集各占 1%；对于数据量过百万的应用，训练集可以占到 99.5%，验证和测试集各占 0.25%。

建议尽量确保验证集和训练集的数据来自同一分布。



### 偏差，方差

高偏差，称为“欠拟合”；方差较高，数据过度拟合。

假定训练集误差是 1%，而验证集误差是 11%，可以看出训练集设置得非常好，而验证集设置相对较差，可能过度拟合了训练集。在某种程度上，验证集并没有充分利用交叉验证集的作用，称之为“高方差”。

假定训练集误差是 15%，而验证集误差是 16%，算法并没有在训练集中得到很好训练，就是数据欠拟合，可以说这种算法偏差比较高。相反，它对于验证集产生的结果却是合理的。

最优误差被称为贝叶斯误差，如果它为 15%，我们再看训练误差 15%，验证误差 16%的这个分类器，偏差不高，方差也非常低。



### 机器学习基础

初始模型训练完成后，如果偏差较高，需要选择一个新的网络，比如含有更多隐藏层或隐藏单元的网络，或花费更多时间来训练网络，或尝试更先进的优化算法。

一旦偏差降低到可以接受的数值，需要检查方差有没有问题。要查看验证集性能。如果方差高，最好的解决方法是采用更多数据；无法获得时，尝试通过正则化来减少过拟合。

只要正则适度，通常构建一个更大的网络便可以在不影响方差的情况下减少偏差；而采用更多数据，通常可以在不过多影响偏差的同时，减少方差。



### 正则化

深度学习可能存在过拟合问题——高方差，有两种解决方法，一是正则化，二是准备更多数据。

将逻辑回归模型 L2 正则化，lambda/(2m) 乘以 w 范数的平方，w 欧几里得范数的平方等于 Wj（j 值从 1 到 n）平方的和。

如果使用 L1 正则化，w 最终会是稀疏的，也就是说 w 向量化中有很多 0。有人说这样有利于压缩模型，存储模型所占用的内存更少。但这并不是 L1 正则化的目的，因而训练网络时，越来越倾向于使用 L2 正则化。

lambda 是正则化参数，通常使用验证集或交叉验证集来配置。需要考虑训练集之间的权衡，把参数设置为较小值，可以避免过拟合。

神经网络的 L2 正则化：矩阵的平方范数被定义为矩阵中所有元素的平方求和，该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标 F 标注。

L2 正则化有时被称为“权重衰减”。



### 为什么正则化有利于预防过拟合呢？

如果正则化 lambda 设置得足够大，权重矩阵 W 被设置为接近于 0 的值，直观理解就是把多隐藏单元的权重设为 0，于是基本上消除了这些隐藏单元的许多影响。这种情况，被大大简化了的神经网络会变成一个很小的网络，可是深度却很大，会使这个网络从过度拟合的状态更接近高偏差状态。但是 lambda 会存在一个中间值。



### dropout 正则化

随机失活，遍历网络的每一层，并设置消除神经网络中结点的概率。

如何实施 dropout 呢？最常用的方法是 inverted dropout（反向随机失活）：keep-prob，表示保留某个隐藏单元的概率。用 python 实现该算法，d 是一个布尔型数组，值为 true 和 false，而不是 0 和 1，乘法运算依然有效。最后向外扩展 a，用它除以 keep-prob 参数，是为了不影响 z 的期望值。

在测试阶段不使用 dropout 函数，因为不期望输出结果是随机的。



### 理解 dropout

dropout 会压缩权重；L2 对不同权重的衰减是不同的，它取决于激活函数倍增的大小。

直观认识：每个结点不愿意给任何一个输入加上太多权重，因为它可能会被删除。

不同层的 keep-prob 可以变化。keep-prob 的值是 1，意味着保留所有单元，是非常常用的输入值；对于有可能出现过拟合，且含有诸多参数的层，可以把 keep-prob 设置成比较小的值。

在计算机视觉领域，经常使用 dropout。因为通常没有足够的数据，所以一直存在过拟合。

dropout 一大缺点就是代价函数 J 不再被明确定义。常关闭 dropout 函数，将 keep-prob 的值设为 1，运行代码，确保 J 单调递减，然后打开 dropout 函数。



### 其他正则化方法

**数据扩增**

例如，随意翻转图片，并把它添加到训练集；也可以随意剪裁图片。

**early stopping**

验证集误差通常会先呈下降趋势，然后在某个节点处开始上升。

还未在神经网络上运行太多迭代过程的时候，参数 w 接近 0，因为随机初始化 w 值时，可能都是较小的随机值。在迭代和训练过程中 w 的值会越来越大，所以 early stopping 要做的就是在中间点停止迭代过程。

机器学习过程包括几个步骤，其中一步是选择一个算法来优化代价函数 J，如梯度下降，但优化后，不想发生过拟合，也有一些工具可以解决该问题，如正则化、扩增数据等。

超级参数激增，选出可行的算法变得越来越复杂。如果我们用一组工具优化代价函数 J，机器学习就会变得更简单。在重点优化代价函数 J 时，只需要留意 w 和 b，其他的不用关注。然后，预防过拟合用另一套工具来实现，这个原理有时被称为“正交化”。early stopping 的主要缺点就是不能独立地处理这两个问题。因为提早停止梯度下降，所以代价函数 J 的值可能不够小；同时，又希望不出现过拟合，没有采用不同的方式来解决这两个问题，而是用同一种方法同时解决两个问题。

如果不用 early stopping，另一种方法就是 L2 正则化，训练神经网络的时间就可能很长，这导致超级参数搜索空间更容易分解，也更容易搜索；但缺点在于，必须尝试很多正则化参数 lambda 的值。



### 归一化输入

归一化需要两个步骤：移动训练集，直到它完成零均值化；归一化方差。

用相同的参数归一化训练集和测试集。

当参数在非常不同的取值范围内，对优化算法非常不利。归一化后，确保所有特征都在相似范围内，通常可以帮助学习算法运行得更快。



### 梯度消失/梯度爆炸

也就是训练神经网络的时候，导数有时会变得非常大，或者非常小。



### 神经网络的权重初始化

tanh 函数使用常量 1，称为 Xavier 初始化；ReLU 函数使用常量 2.

如果想要添加方差，可以给公式添加一个乘数参数。对于调优过程中产生的问题，虽然调优该参数能起到一定作用，但相比其他超级参数的重要性，它的优先级较低。



### 梯度的数值逼近

执行梯度检验时，使用双边误差，而不使用单边误差，因为它不够准确。



### 梯度检验

欧几里得范数，即欧氏距离，是误差平方之和，然后求平方根。

如果梯度检验值在 10 的负 7 次方或者更小，意味着导数逼近有可能是正确的。



### 梯度检验应用的注意事项

首先，不在训练中使用梯度检验，它只用于调试。第二点，如果算法的梯度检验失败，要检查所有项，试着找出 bug。第三点，在实施梯度检验时，如果使用正则化，请注意正则项。第四点，梯度检验不能与 dropout 同时使用。





## 优化算法

### Mini-batch 梯度下降

把训练集分割为小一点的子集训练，这些子集被取名为 mini-batch。

1 epoch，意味着只是一次遍历了训练集。



### 理解 mini-batch 梯度下降法

使用 mini-batch 梯度下降法，如果作出成本函数在整个过程中的图，并不是每次迭代都是下降的；走向朝下，但有更多的噪声。

如果 mini-batch 大小为 1，叫做随机梯度下降法，永远不会收敛，一直在最小值附近波动，通过减小学习率，噪声会被改善或有所减小，但效率过于低下。如果大小为 m，成为 batch 梯度下降法，每次迭代需要处理大量训练样本，主要弊端在于单次迭代耗时太长。

首先，如果训练集较小，直接使用 batch 梯度下降法，如小于 2000 个样本。样本数目较大的话，一般 mini-batch 为 64 到 512。考虑到电脑内存设置和使用的方式，mini-batch 大小是 2 的 n 次方，代码会运行得快一些。最后需要注意的是，mini-batch 需和 CPU/GPU 内存相符。



### 指数加权平均数

在统计学中，被称为指数加权移动平均值。

beta  值较大，得到的曲线平坦一些，缺点是曲线进一步右移。



### 理解指数加权平均数

指数加权平均数的好处之一在于，占用单行数字的存储和内存。



### 指数加权平均的偏差修正

减少初始时的偏差。



### 动量梯度下降法

Momentum，即动量梯度下降法。运行速度几乎总是快于标准的梯度下降算法。基本想法是计算梯度的指数加权平均数，并利用该梯度更新权重，从而减缓梯度下降的幅度。

beta 最常用的值是 0.9。实际中，使用梯度下降法或动量梯度下降法时，不会受到偏差修正的困扰。



### RMSprop

root mean square prop，微分平方的加权平均数。

可以使用一个更大的学习率 alpha，加快学习，且无须在纵轴上偏离。

为了确保数值稳定，要在分母上加一个很小很小的数，10 的 -8 次方是个不错的选择。



### Adam 优化算法

Adaptive Moment Estimation，将 Momentum 和 RMSprop 结合在一起。一般使用 Adam 算法时，要进行偏差修正。

分为第一矩，第二矩。其中，RMSprop 中的 beta 推荐使用 0.999，Momentum 中推荐使用 0.9。



### 学习率衰减

加快学习算法的一个办法是随时间慢慢减少学习率，称为学习率衰减。

alpha = alpha0/(1+decayrate*epoch-num)（decay-rate 为衰减率，epoch-num 为代数，alpha0 为初始学习率），其中，衰减率为另一个需要调整的超参数。



### 局部最优的问题

神经网络中，梯度为零的点通常不是局部最优点。实际上，成本函数的零梯度点，通常是鞍点。前提是较大的神经网络，大量参数，并且成本函数 J 定义在较高的维度空间。

如果局部最优不是问题，那么问题是什么？平稳段会减缓学习。在这种情况下，更成熟的优化算法，如 Adam，能够加快速度，从而尽早走出平稳段。





## 超参数调试、Batch 正则化和程序框架

### 调试处理

学习速率是需要调试的最重要的超参数。其次是 Momentum 参数 beta、mini-batch 大小和隐藏单元。然后是层数和学习率衰减。

随机取值。

从粗糙到精细，即聚焦到更小的方格。



### 为超参数选择合适的范围

用对数标尺搜索超参数，在对数轴上均匀随机取点。



### 超参数训练的实践：Pandas VS Caviar

每隔几个月，重新测试或评估超参数。

由所拥有的计算资源决定使用哪种方式。



### 归一化网络的激活函数

归一化 z。

训练输入和这些隐藏单元值的区别是，不想让隐藏单元值必须是平均值 0 和方差 1。



### 将 Batch Norm 拟合进神经网络

使用 Batch 归一化，其实可以消除参数 b。



### Batch Norm 为什么奏效？

使权重比网络更滞后或更深层，它限制了前层的参数更新对数值分布程度的影响。

有轻微的正则化效果，因为给隐藏单元增加了噪音，使得后部单元不过分依赖任何一个隐藏单元。

一次只能处理一个 mini-batch 数据。



### Softmax 回归

有一种逻辑回归的一般形式，叫做 Softmax 回归。

计算幂得到临时变量，然后归一化，称为 Softmax 激活函数。输入一个向量，输出一个向量。



### 训练一个 Softmax 分类器

Softmax 这个名称的来源是与 hardmax 对比。hardmax 函数会观察 z 的元素，然后在最大元素的位置上放 1，其他位置放 0。

损失函数所做的就是找到训练集中的真实类别，然后使该类别相应的的概率尽可能地高。





## 机器学习策略

### 正交化

如果在开发集上做得不错，但测试集不行，可能意味着对开发集过拟合了，需要回退一步，使用更大的开发集。



### 单一数字评估指标

查准率（precision）。如果分类器 A 有 95% 的查准率，意味着分类器说图片有猫的时候，有 95% 的概率真的有猫。

查全率（recall）。如果分类器 A 查全率是 90%，意味着对所有猫的图片，分类器准确分辨出了其中的 90%。

F1 分数结合查准率和查全率，是二者的调和平均数。



### 满足和优化指标

如果考虑 N 个指标，有时选择其中一个作为优化指标，剩下 N-1 个都是满足指标，意味着只要它们达到一定阈值即可。



### 训练/开发/测试集划分

尝试很多思路，用训练集训练不同的模型，使用开发集来评估不同的思路，选择一个，然后不断迭代去改善开发集的性能，直到最后得到一个满意的成本，再用测试集去评估。



### 开发集和测试集的大小

对于某些应用，不需要对系统性能有置信度很高的评估，只需要训练集和开发集。



### 为什么是人的表现？

贝叶斯最优错误率，即理论上可能达到的最优错误率。



### 可避免偏差

贝叶斯错误率和训练错误率之间的差值，称为可避免偏差。理论上是不可能超过贝叶斯错误率的，除非过拟合。



### 改善模型的表现

减少可避免偏差，建议策略：使用规模更大的模型；使用更好的优化算法，如加入 Momentum、RMSprop 或 Adam；改变激活函数；改变层数或隐藏单位数；试用其他架构，如循环神经网络和卷积神经网络。

减少方差，建议策略：收集更多数据；使用正则化；试用不同的神经网络架构。



### 进行误差分析

寻找一组错误样本，观察错误标记的样本，统计属于不同错误类型的错误数量。



### 清楚标注错误的数据

深度学习算法对于训练集中的随机错误是相当健壮的，而对系统性的错误并非如此。

修正手段要同时作用到开发集和测试集上。



### 快速搭建第一个系统，并进行迭代

搭建快速而粗糙的实现，然后用它做偏/方差和错误分析，用分析结果确定下一步优先要做的方向。



### 不匹配数据划分的偏差和方差

随机打散训练集，分出一部分作为训练-开发集。

只在训练集训练神经网络，不在训练-开发集上跑后向传播。为了进行误差分析，需要考察训练集误差、训练-开发集误差及开发集误差。

假设训练误差为 1%，训练-开发误差为 1.5%，开发集错误率为 10%，这是数据不匹配问题。

假设训练集错误率是 10%，训练-开发集误差为 11%，开发集误差为 20%。存在两个问题：第一，可避免偏差相当高（贝叶斯错误率接近 0%）；第二，方差很小，但数据不匹配很严重。



### 定位数据不匹配

可以利用的一种技术是人工数据合成，从而快速制造更多的训练数据，但有可能存在过拟合的问题。

The quick brown fox jumps over the lazy dog，这个句子在 AI 测试中经常使用，因为它包含了从 a 到 z 所有字母。



### 迁移学习

初始化最后一层权重。

经验规则是，如果数据集较小，只训练输出层前的最后一两层；如果数据集较大，可以重新训练网络中的所有参数。

预训练和微调。

假设可以把图像识别中学到的知识迁移到放射科诊断上，为什么这样做有效果呢？有很多低层次特征，如边缘检测、曲线检测、阳性对象检测等，从非常大的图像识别数据库中习得的能力有助于放射科诊断网络学习得更快一些，或需要更少的数据。

什么时候迁移学习是有意义的？从任务 A 学习并迁移一些知识到任务 B，且当二者具有同样的输入。若任务 A 的数据比任务 B 多得多，任务 A 的低层次特征可以帮助任务 B 的学习时，意义更大。



### 多任务学习

在迁移学习中，步骤是串行的；在多任务学习中，试图让单个神经网络同时做几件事情，希望这里的每个任务都能帮到其他所有任务。

只对有标签的值求和。

多任务学习什么时候有意义？第一，训练的任务组可以共用低层次特征；第二，其余任务可以提供数据或知识，即其他任务加起来必须要有比单个任务大得多的数据量。

多任务学习会降低性能的唯一情况就是神经网络还不够大。



### 什么是端到端的深度学习？

以前的数据处理系统或学习系统，需要多个阶段的处理。端到端的深度学习就是忽略所有不同的阶段，用单个神经网络替代它。

数据集较小时，传统流水线方法通常做得更好；需要大数据集，才能让端到端方法发挥真正的作用。

端到端深度学习在机器翻译领域非常好用。



### 是否要使用端到端的深度学习？

~~~
优点：
数据说话；
手工设计组件少，简化设计工作流程。

缺点：
需要大量数据；
排除了可能有用的手工设计组件。
~~~





## 卷积神经网络

### 边缘检测示例

卷积运算是卷积神经网络最基本的组成部分。将过滤器覆盖在输入图像，进行元素乘法，然后将矩阵每个元素相加。



### 更多边缘检测内容

Sobel 过滤器，增加中间一行元素的权重，使得结果的鲁棒性更好。

将矩阵的所有数字都设置为参数，通过数据反馈，让神经网络自动去学习它们。



### Padding

存在两个缺点：第一，每次卷积操作，图像就会缩小；第二，在角落或者边缘区域的像素点利用较少，意味丢掉了图像边缘位置的许多信息。

解决方法：在卷积操作之前填充图像。习惯上，可以用 0 填充。

Valid 卷积：不填充 。

Same 卷积：填充后，输出和输入大小是一样的，p = (f - 1) / 2。

在计算机视觉中，f 通常是奇数。原因：第一，如果是偶数，只能使用一些不对称填充；第二，存在中心点，便于指出过滤器的位置。



### 卷积步长

如果使用一个 f X f 的过滤器卷积一个 n X n 的图像，padding 为 p，步幅为 s，输出图像长度为 (n + 2p - f) / s +1。如果商不是整数，向下取整。

惯例是，过滤器必须完全处于图像中或填充后的图像区域才能输出相应结果。 

互相关 v.s. 卷积：按照机器学习的惯例，通常把不进行翻转操作的称为卷积。



### 三维卷积

检测 RGB 彩色图像特征。假设彩色图像是 6X6X3，其中，3 指三个颜色通道，可以想象成三个图像的堆叠。这样，过滤器也有三层，对应红、绿、蓝通道。

图像的通道数必须和过滤器的通道数匹配。

输出的通道数（三维立方体深度）等于需检测的特征数。



### 单层卷积网络

不同的过滤器各自形成卷积神经网络层。对应于每一层，增加偏差。通过 python 的广播机制给该层的所有元素加上同一偏差，然后应用非线性函数。

无论输入图片有多大，参数个数始终不变。这就是卷积神经网络的一个特征——“避免过拟合”。



### 简单卷积网络示例

最终可以对卷积进行处理，将其平滑或展开。

随着神经网络计算深度的不断增加，开始时的图像需要大一些。

一个典型的卷积神经网络通常有三层。一个是卷积层，常用 CONV 标注；一个是池化层，称为 POOL；最后一个是全连接层，用 FC 表示。

虽然仅用卷积层也可能构建出很好的神经网络，但大部分神经网络架构师依然会添加池化层和全连接层。幸运的是，池化层和全连接层比卷积层更容易设计。



### 池化层

除了卷积层，卷积网络也经常使用池化层来缩减模型大小，提高计算速度，同时提高所提取特征的鲁棒性。

最大池化（分别对每个通道执行）：只要在任何一个象限内提取到某个特征，它都会保留在最大化的池化输出里。所以最大化运算的实际作用是，如果在过滤器中提取到某个特征，那么保留其最大值。如果没有提取到这个特征，其中的最大值还是很小。

平均池化：平均值。

目前，最大池化比平均池化更常用。

池化的超参数包括过滤器大小 f 和 步幅 s，常用的值为 f=2，s=2，其效果相当于高度和宽度缩减一半。

大部分情况下，最大池化很少用 padding。

池化特点之一是，它有一组超参数，但并没有参数需要学习。一旦确定了 f 和 s，它就是一个固定运算，梯度下降无需改变任何值。计算神经网络某一层的静态属性。



### 卷积神经网络示例

在计算神经网络有多少层时，通常只统计有权重和参数的层，而池化层没有权重和参数，只有一些超参数。

全连接层，类似于单神经网络层。

**注意点：池化层没有参数；卷积层参数相对较少；许多参数存在于全连接层。**

随着神经网络的加深，激活值尺寸会逐渐变小；如果激活值尺寸下降太快，会影响神经网络性能。



### 为什么使用卷积？

和只用全连接层相比，卷积层的两个主要优势在于：参数共享和稀疏连接。

参数共享：每个特征检测器及输出都可以在输入图片的不同区域中使用相同的参数。

稀疏连接：其他像素值不会对输出产生任何影响。

卷积神经网络善于捕捉平移不变。





## 深度卷积网络：实例探究

### 为什么要进行实例探究？

ResNet，又称残差网络，训练了一个深达 152 层的神经网络。



### 经典网络

LeNet-5：针对灰度图片训练。一个或多个卷积层后面跟一个池化层，又是若干个卷积层再接一个池化层，然后是全连接层，最后是输出。池化层后使用了 sigmoid 函数。

AlexNet：用于训练图像和数据集时，能够处理非常相似的基本构造模块。使用 ReLU 激活函数。在写这篇论文的时候，GPU 的处理速度还比较慢，所以采用了非常复杂的方法在两个 GPU 上进行训练。大致原理是，把层分别拆分到两个不同的 GPU 上，同时还专门有一个方法用于两个 GPU 交流。

VGG：简化了神经网络结构。使用 same 卷积。



### 残差网络

跳跃连接：从某一网络层获取激活，然后迅速反馈给另一层，甚至是神经网络的更深层。

可以利用跳跃连接构建能够训练深度网络的 ResNets。

ResNets 是由残差块构建的。

残差块：信息直接到达神经网络深层，不再沿着主路径传递。

使用标准优化算法训练一个普通网络，如梯度下降法，如果没有这些捷径或跳跃连接，随着网络深度加深，训练错误会先减少，然后增多。然而，理论上网络深度越深越好。但实际上，如果没有残差网络，对于普通网络来说，深度越深意味着优化算法越难训练。

使用 ResNets 确实有助于解决梯度消失和梯度爆炸问题。



### 残差网络为什么有用？

主要原因是这些残差块学习恒等函数非常容易，可以确定网络性能不会受到影响，或者说至少不会降低网络的效率。

ResNets 使用了许多 same 卷积。之所以能实现跳跃连接，是因为 same 卷积保留了维度，所以容易得出捷径连接，并输出这两个相同维度的向量。

ResNets 类似于其他很多网络，会有很多卷积层，偶尔会有池化层等。

普通网络和 REsNets 网络常用的结构是：卷积层-卷积层-卷积层-池化层-卷积层-卷积层-卷积层-池化层……直到最后，有一个通过 softmax 进行预测的全连接层。



### 网络中的网络以及 1X1 卷积

1X1 卷积可以从根本上理解为对不同位置应用一个全连接层，有时也被称为 Network in Network。

功能：给神经网络添加了一个非线性函数，从而减少或保持输入层中的通道数量不变。



### 谷歌 Inception 网络简介

Inception 网络或 Inception 层的作用就是代替人工来确定卷积层中的过滤器类型、是否需要创建卷积层或池化层等，虽然网络架构因此变得更加复杂，但网络表现却非常好。可以给网络添加参数的所有可能值，然后把这些输出连接起来，让网络自己学习它需要什么样的参数，采用哪些过滤器的组合等。

瓶颈层是网络中最小的部分，先缩小网络表示，然后再扩大它。

通过使用 1X1 卷积来构建瓶颈层，从而大大降低计算成本。

大幅缩小表示层规模会不会影响神经网络的性能？只要合理构建瓶颈层，既可以显著缩小表示层规模，又不会降低网络性能。



### Inception 网络

Inception 网络确保了即便是隐藏单元和中间层也参与了特征计算，也可以预测图片的分类。它在 Inception 网络中，起到了一种调整的效果，能够防止网络发生过拟合。



### 迁移学习

通过使用其他人预训练的权重，极有可能得到很好的性能，即使只有一个小的数据集。

不同的深度学习编程框架有不同的方式，允许指定是否训练特定层的参数。

冻结网络中所有层的参数，只需要训练和你的 Softmax 层有关的参数。

由于前面的层都冻结了，相当于一个固定的函数，不需要改变。存在一个加速训练的技巧：取任意输入图像 X，然后计算它的某个特征向量，用其做预测。把训练集中所有样本冻结层的激活值进行预计算，然后存储到硬盘里，在此之上训练 Softmax 分类器。优点是不需要每次遍历训练集再重新计算激活值 。（适合于任务只有一个很小的数据集）

数据越多，需要冻结的层数越少，能够训练的层数越多。如果有大量数据，应该做的就是用开源的网络和它的权重，把其当做初始化，然后训练整个网络。



### 数据扩充

垂直镜像对称。

随机剪裁。

彩色转换。给定图片，在 R、G 和 B 三个通道上，基于某些分布加上不同的失真值。同时，使得学习算法对照片的颜色更具鲁棒性。

PCA（主成分分析）颜色增强：比如图片呈紫色，即主要含有红色和蓝色，绿色很少；该算法会对红色和蓝色增减很多，绿色变化相对少一点，从而使总体的颜色保持一致。



### 计算机视觉现状

Benchmark 基准测试：一个评价方式，主要测试负载的执行时间、传输速度、吞吐量和资源占用率等。

基准测试小技巧：第一，集成。独立训练几个神经网络，并平均它们的输出。第二，multi-crop at test time，将数据扩充应用到测试图像。





## 目标检测

### 目标定位

让神经网络再多输出 4 个数字，是被监测对象的边界框的参数化表示。



### 目标检测

滑动窗口目标检测。缺点：选用步幅很大，会减少输入卷积网络的窗口个数，但粗糙间隔尺寸会影响性能；如果采用小粒度或小步幅，传递给卷积网络的小窗口会特别多，意味着超高的计算成本。



### 卷积的滑动窗口实现



